\chapter{Numerical Solvers in Physics Based Simulation} \label{Chapter:Solver}
In the pipeline of physics based simulation, the complexity of majority of the stages are simply $O(N)$, where $N$ is the number of unknowns, while the complexity of the solve stage can be as much as $O(N^2)$. For high resolution simulation, the solve stage can often take over 90\% of the simulation time. There are two major categories of numerical solvers, direct solvers and iterative solvers. 

Direct solvers provide solution at the precision of the numerical limit, and they generally operates in sparse matrix format (for instance Compressed Sparse Column). Therefore they can be used in arbitrary discretization and problems, but they have the minimal cost of $O(N^2)$ and memory footprint of $O(N^{\frac{4}{3}})$ or higher. This super-linear cost makes direct solvers infeasible for high resolution simulations. But for small scale simulations, direct solver algorithms such as Cholesky factorization are attractive for their accuracy and robustness. 

Iterative solvers, on the other hand, as the name suggests, provides a solution that converges to the exact solution with each iteration. Some iterative solvers, such as Conjugate Gradient Method[\cite{nocedal2006conjugate}] and Generalized Minimal Residual algorithm\cite{saad1986gmres}], mathematically speaking, can achieve the exact solution at the $N$th iteration. At the cost of $O(N)$ per iteration, it will give the exact solution with $O(N^2)$ cost. But in practice, due to numerical drift, those algorithm will require restarts for large problems, and can't achieve the exact solution to numerical limit. 

For large problems, an iterative solver is preferred over direct solver for two major reasons: 1. the memory footprint of an iterative solver is usually $O(N)$, which allows larger problems to fit into memory. 2. Though iterative solve can potentially takes longer to achieve solution at numerical limit, the option to terminate early in exchange for an approximate solution is attractable when computation time is limited. But at higher and higher resolution, iterative solvers can take considerably longer to converge and sometimes even stagnate (See results in Chapter \ref{Chapter:Elasticity}). If terminates too early, the poorly approximated solution can lead to unphysical results, due to that the solution can not satisfies the governing equation.

\section{Multigrid Method Overview}
Multigrid method was proposed in work [\cite{brandt1977multi}]. The multigrid fundamental idea lies as follows: we start with a set of grids $G^0$,$G^1$,...,$G^M$, which are all discretization of the same domain $\Omega$. With higher level consists of a coarser elements. In our uniform grid discretization, int 1D, if the nodes in $G^0$ lies on points $(0,h,2h,3h,4h...)$. We can construct the next level $G^1$ as nodes on points $(0,2h,4h,6h,8h...)$.

If our continuous PDE of the boundary value problem takes the form:
\begin{equation}
LU(x) = F(x)\text{, in }\Omega,  \Lambda U(x) = \Phi(x)\text{, on the boundary }\partial\Omega
\end{equation}
Here, $L$ and $\Lambda$ are the differential operators that are on the interior of the domain and the boundary respectively. $U(x)$ is the solution we seek. $F(x)$ and $\Phi(x)$ are the loading condition that are given. We can now write the discretized PDE at each level $k$ as:
\begin{equation}\label{equ:mg_operator}
L^kU^k(x) = F^k(x)\text{, for} x \in G^k,  \Lambda^k U^k(x) = \Phi^k(x)\text{, for} x\in\partial G^k 
\end{equation}
Now, $L^k$ and $\Lambda^k$ are linear operators and can be perceived as matrices. We are interested in acquiring the solution at the finest level. The main idea is, if at $k$th level, the solution $U^k(x)$ is a good approximation to the continuous $U(x)$. We can use the $k$th level solution as a potent guess for the $k-1$ level. Then apply a correction routine called \textbf{smoother} that is $O(|G^{k-1}|)$ cost to bring the guess close to solution. This process, sometimes refers to as V-cycle or W-cycle, is repeated until $U^0(x)$ is considered close enough to the solution. 

Multigrid has the property that each iteration can reduce the error by at least a constant factor $W$. That is $\frac{|\mathbf{e}_{i+1}|}{|\mathbf{e}_{i}|}<W$. Here we take $|\mathbf{e}|$ as the infinite norm of $\mathbf{e}$. What the value of $W$ is depends on the PDE, how the discretization of each level is constructed, and how potent the smoother is. The convergence property of multigrid will  be elaborate more in later section. But we can see that if we seek to reduce the error by 6 orders of magnitude, we will need $-\log_W 10^6$ iterations to converge, which is a constant. For instance if $W = 0.5$, it will take at most 20 iterations to converge. But, of course, in bad cases, for instance, when $W = 0.99$, it will take about 1400 iterations.

Now we have introduced some fundamental multigrid concepts, let's take a deep look at multigrid implementations.
\section{Construction of the Hierarchy}
For constructing the discretized hierarchal operators in equations \ref{equ:mg_operator}, there are two common ways, geometric coarsening and Galerkin coarsening. In work [\cite{zhu2010efficient}, \cite{mcadams2010parallel}], geometric coarsening was used to construct hierarchal operators. For the domain decomposition solver, geometric coarsening based multigrid is used as it's building block, see Chapter \ref{Chapter:DD}. 

But in many cases, only the top level operator is provided to the solver, and the underlining PDE is inaccessible, re-discretization of the PDE with a coarser grid(or mesh) is not an option, in those cases, Galerkin coarsening is usually used for constructing the hierarchy, such as in work [\cite{dendy1982black}, \cite{brezina2001algebraic}, \cite{dohrmann2007interpolation}]. This is also our method for the multigrid construction in our SIMD-optimized solver for topology optimization.  

When the underlining PDE is unavailable, there is the possibility to acquire the geometric coarsened operator through homogenization of Galerkin coarsened operator which is demonstrated in work [\cite{moulton1998black}]. But this is aspect is not the focus of this work. Therefore I will leave out here the discussion of homogenization.
\section{Geometric Coarsening}
\label{sec:geo_coarsen}
Geometric coarsening refers to the method of constructing the Multigrid hierarchy through re-discretization with different grid size. It has been proven effective for both homogeneous Poisson[\cite{mcadams2010parallel}] and homogeneous elasticity[\cite{zhu2010efficient}]. Here I will take finite difference Poisson discretization as an example to demonstrates the geometric coarsening principles and its limitation. For more details, I would refer the reader to the work[\cite{mcadams2010parallel}].

The continuous PDE of homogeneous Poisson can be written as:
\begin{equation}
\Delta p\mathbf{x} = f\mathbf{x} \text{ in } \Omega \in \mathcal{R}^3
\end{equation}
$$
p(\mathbf{x}) = \alpha(\mathbf{x}) \text{ on } \Gamma_D\text{, } p_n(\mathbf{x}) = \beta(\mathbf{x}) \text{ on } \Gamma_N
$$

\begin{figure}[t]
\includegraphics[width=4cm]{Poisson_Stencil}
\centering
\caption{A finite difference Poisson stencil on the boundary. White cells are interiors cell, i.e. they are degrees of freedom. Blue cell are Dirichlet condition, or essential conditions, that their value are prescribed. Grey cells are exterior cells that are not part of the domain $\Omega$. The face between gray and white cell describes a Nuemann condition, or a nature boundary.}
\label{fig:PoissonBoundary}
\end{figure}

The finite difference discretization on a uniform grid of size $h$, samples points we denote as $\{i,j,k\} \in \mathcal{N}^3$ that corresponding to geometric location $\{ih,jh,kh\} \in \Omega$. The finite difference discretized operator for the interior can be written as:
\begin{equation}
\sum_{i',j',k'\in \mathcal{N}_{[i,j,k]}}\frac{p_{[i',j',k']}-p_{[i,j,k]}}{h^2} = f_{[i,j,k]}
\end{equation}
$$
\mathcal{N}_{[i,j,k]} = \{(i\pm1,j,k),(i,j,k\pm1),(i,j,k\pm1)\}
$$
It is a 2nd order accurate discretization as we can write:
$$
\frac{\partial^2 p}{\partial x^2}(x_0) = \frac{p(x_0)-p(x_0)}{h^2} + O(h^2)
$$
As for boundary conditions, either Nuemann/nature boundary or Dirichlet/essential boundary, we can discretize the operator as shown in Figure ~\ref{fig:PoissonBoundary}. It is only 2nd order accurate, if the boundary condition is perfect aligned with cell. But generally multigrid is used as preconditioner. For that purpose, this discretization is proven to be sufficient for preconditioner even with non-cell align boundaries[\cite{aanjaneya2017power}]. 

But even though we may assume that the boundary conditions are cell aligned at the finest level, this may not holds true for the coarse levels, when the cell size doubles. Figure ~\ref{fig:GeometricCoarsening} demonstrates the heuristic for coarsening boundary conditions. Note that after coarsening, the coarse level discretization may not be of the same order of accuracy to the continuous PDE as the finest level at the boundaries. This inconsistent boundary conditions break the principle that the each level of the multigrid hierarchy should be the discretization of the \textbf{same} continuous PDE. But to compensate this discrepancy, we can introduce additional boundary smoothing to stabilize the multigrid.
\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Dirichlet_Coarsening}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{Nuemann_Coarsening}
\end{minipage}
  \caption{Geometric coarsening of the simulation domain. Left: if one of the cells is Dirichlet cell, we will coarsen it as Dirichlet cell at the coarse level, Right: otherwise if one of the cells is interior cell, it is coarsened as an interior cell at the coarse level.}
  \label{fig:GeometricCoarsening}
\end{figure}
\section{Garlerkin Coarsening}
Galerkin coarsening, also sometimes refers to algebraic coarsening, is usually the preferred method for handling complex geometries or heterogeneous domains. It was summarized in work [\cite{brandt1986algebraic}]. Many algebraic algorithms, assumes only the top level linear operator is available in matrix form. In such case, the first step of the algorithm is to select the coarse grid DOF based on the matrix given. In this dissertation, we choose the coarsened DOF the same way as in geometric multigrid, that is, coarse DOF coincide with every other fine DOF in each dimension. If readers are interested in the coarse grid selection, I will direct to work [\cite{vanvek1996algebraic},\cite{brandt1986algebraic},\cite{griebel2003algebraic}].

\begin{figure}[t]
\centering
\includegraphics[width=6cm]{Interpolation}
  \caption{Multilinear prolongation operator in 1D. The value from corresponding fine nodes is copied from the coarse node, it they coincide in the domain, otherwise the fine node value is interpolated from the two closest coarse nodes each with weight 0.5.}
  \label{fig:MultilinearInterpolation}
\end{figure}

After the coarse grid selection, a \textbf{prolongation} operator is constructed. It is a linear operator that interpolates fine DOF values from coarse DOF. The most common prolongation operator is multi-linear interpolation. In 1D, it can be illustrated as Figure~\ref{fig:MultilinearInterpolation}.

We can also write this in matrix form:
$$
\mathbf{P} = \begin{bmatrix}
1 & 0 & 0 \\
0.5 & 0.5 & 0 \\ 
0 & 1 & 0 \\
0 & 0.5 & 0.5 \\
0 & 0 & 1
\end{bmatrix}
$$
The interpolation process is then:
\begin{equation}
\mathbf{u}^f = \mathbf{P} \mathbf{u}^c
\end{equation}
Here $\mathbf{x}^f$ is the fine grid value and $x^c$ is the coarse grid value. In the case of multi-level hierarchy, we can denote the prolongation operator from level $k+1$ to level $k$ as $\mathbf{P}^k$. Therefore:
\begin{equation}
\mathbf{u}^k = \mathbf{P}^k \mathbf{u}^{k+1}
\end{equation}
Here, $P$ is a rank deficient matrix, which means when prolongate from a coarse level to a fine level, the vector at fine level has a higher dimension and is in general not the correct solution of the fine level discretization. Therefore, smoother is the process used to correct this error. This means smoother should be selected based on error that is invisible to coarse grid, and vise versa, coarse grid should correct error mode that hard to reduce by smoother. In the next section we will discuss more this duality.  Multilinear interpolation has proven to be effective prolongation operator for homogeneous and isotropic PDEs[\cite{mcadams2010parallel}][\cite{aanjaneya2017power}][\cite{zhu2010efficient}]. It is also used by geometric multigrid. 

Now we have defined the prolongation operator $\mathbf{P}^k$. The Galerkin coarsening process construct the $k+1$ operator $\mathbf{L}^{k+1}$ as follows:
\begin{equation}
\mathbf{L}^{k+1} = (\mathbf{P}^k)^T \mathbf{L}^{k}\mathbf{P}^k
\end{equation}
The coarse level correction can be written as(for solving $\mathbf{L}\mathbf{u} = \mathbf{f}$):
\begin{align*}
\mathbf{r} &= \mathbf{b} - \mathbf{L}\mathbf{u}_0 \\
\mathbf{b}^c &= \mathbf{P}^T\mathbf{r} \\
\mathbf{u}^c &= (\mathbf{L}^c)^{-1} \mathbf{b}^c \\
\mathbf{u}^* &= \mathbf{u}_0 + \mathbf{P}\mathbf{u}^c\\
\end{align*}
Here, superscript c indicates unknowns and matrices from the coarse grid. $\mathbf{u}_0$ is the current guess of the fine level solution.$\mathbf{u}^*$ is the new guess of the fine level solution after the coarse correction.
\begin{lem}\label{lemma:projection}The coarse grid correction of Galerkin coarsened hierarchy is a projection.\end{lem}
\begin{proof}
This lemma suggests when doing coarse correction twice, the second correction process will create the same result as first one.

If we start with $\mathbf{u}^* = \mathbf{u}_0 + \mathbf{P}\mathbf{u}^c$.
\begin{align*}
\mathbf{r}^* &= \mathbf{b} - \mathbf{L}\mathbf{u}^* \\
\mathbf{b}^{*c} &= \mathbf{P}^T\mathbf{r}^* \\
&= \mathbf{P}^T(\mathbf{b} - \mathbf{L}\mathbf{u}^*) \\
&= \mathbf{P}^T(\mathbf{b} - \mathbf{L}(\mathbf{u}_0 + \mathbf{P}\mathbf{u}^c))\\
&= \mathbf{P}^T(\mathbf{b} - \mathbf{L}\mathbf{u}_0) - \mathbf{P}^T\mathbf{L}\mathbf{P}\mathbf{u}^c \\
&= \mathbf{P}^T\mathbf{r} - \mathbf{L}^c\mathbf{u}^c \\
&= \mathbf{b}^c - \mathbf{b}^c \\
&= 0
\end{align*}
We can see that the second correction process will have $0$ as right hand side, therefore no correction will be produced.
\end{proof}
In general, we have $rank(L^{k+1}) < rank(L^{k})$. Lemma \ref{lemma:projection} implies that the coarse grid correction is projecting the fine solution onto a subspace and solve within the subspace. All the error modes $\mathbf{e}$ that has property $\mathbf{P}\mathbf{e} = \mathbf{0}$ will be invisible to the coarse level, and rely on the smoother for correction.
\section{Multigrid V-cycle}
Now we have constructed the hierarchy, let's take a look at a multigrid V-cycle.
\begin{algorithm}[H]
\caption{Multigrid V-cycle}
\label{alg:vcycle}
\begin{algorithmic}
\STATE $\mathbf{u}^0 = \mathbf{0}$
\STATE $\mathbf{r}^0 = \mathbf{b}^0 - \mathbf{L}^0\mathbf{u}^0$
\WHILE{$|\mathbf{r}^0| < threshold$ || max\_iteration has reached}
	\FOR{i := 0 \TO k - 2}
		\STATE smooth($\mathbf{u}^i,\mathbf{L}^i,\mathbf{b}^i$);
		\STATE $\mathbf{r}^i = \mathbf{b}^i - \mathbf{L}^i\mathbf{u}^i$
		\STATE $\mathbf{b}^{i+1} = (\mathbf{P}^i)^T\mathbf{r}^i $
	\ENDFOR
	\STATE $\mathbf{u}^{k-1} = (\mathbf{L}^{k-1})^T \mathbf{b}^{k-1}$	
	\FOR{i := k - 2 \TO 0}
		\STATE $\mathbf{u}^{i} += \mathbf{P}^i\mathbf{u}^{i+1} $
		\STATE smooth($\mathbf{u}^i,\mathbf{L}^i,\mathbf{b}^i$);		
	\ENDFOR
	\STATE $\mathbf{r}^0 = \mathbf{b}^0 - \mathbf{L}^0\mathbf{u}^0$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
The process of rather straight forward: at each level, first, we reduce errors that are (hopefully) invisible to the coarse grid using a smoother. Then the residual of the remaining error is computed and restricted to the coarse level. This process is repeated until  the bottom level is reached where the problem is small enough to be solved using a direct solver. This coarse level correction is than prolongated to finer levels, then the smoother is applied again until this process reaches the top. The multigrid V-cycle can be repeated until satisfactory convergence is reached or the max number of iterations has reached.

This multigrid iteration has an asymptotic convergence, that is, after enough iterations, the ratio between the residual norm at iteration $k$ and iteration $k+1$ will converge to a constant[\cite{brandt1977multi}]. After introducing the smoother in the next section, I will give a more concrete example on what this asymptotic convergence can be for an ideal problem.
\section{Smoother}
Smoother a generally refer to a class of iterative solvers that use only local information, and the cost of each application is comparable to a matrix multiplication. Common smoothers are (but not restricted to) Jacobi method, Gauss-Seidel method, Richardson iteration, and successive over-relaxation method. As the simplest, I will take Richardson iteration as example. Richardson iteration can be written as(using notations from before):
\begin{align*}
\mathbf{r} &= \mathbf{b} - \mathbf{L}\mathbf{u}_k \\
\mathbf{u}_{k+1} &= \mathbf{u}_k + \mathbf{r} \\
\end{align*}
Here $\mathbf{u}_k$ is the solution at iteration $k$. If we define $\mathbf{u}^*$ the solution, that is
$$
\mathbf{L}\mathbf{u}^* = \mathbf{b}
$$
We can write the error of the current solution  as
$$
\mathbf{e}_k = \mathbf{u}^* - \mathbf{u}_k 
$$
And the residual is then
$$
\mathbf{r}_k = \mathbf{L}\mathbf{e}_k
$$
Assuming $\mathbf{L}$ is symmetric positive definite. We write the Eigen vector of $\mathbf{L}$ as $\mathbf{v}_i$, that are orthogonal to each other. Then we can write the error as combination of the Eigen vectors:
$$
\mathbf{e}_k = \sum_i \gamma^k_i\mathbf{v}_i
$$
Here $\gamma^k_i$ are the length when project $\mathbf{e}_k$ onto $\mathbf{v}_i$. We can rewrite the smoothing iteration as:
 \begin{align*}
\mathbf{u}_{k+1} &= \mathbf{u}_k + \mathbf{L}\mathbf{e}_k \\
\mathbf{u}_{k+1} - \mathbf{u}_k &= \mathbf{L}\mathbf{e}_k \\
(\mathbf{u}^* - \mathbf{u}_{k}) - (\mathbf{u}^* - \mathbf{u}_{k + 1}) &= \mathbf{L}\mathbf{e}_k\\
\mathbf{e}_{k} -  \mathbf{e}_{k + 1} &= \mathbf{L}\sum_i \gamma^k_i\mathbf{v}_i\\
\mathbf{e}_{k} -  \mathbf{e}_{k + 1} &= \sum_i \gamma^k_i\mathbf{L}\mathbf{v}_i
\end{align*}
We have $\mathbf{L}\mathbf{v}_i = \lambda_i\mathbf{v}_i$, where$\lambda_i$ is the Eigen value associated with Eigen vector $\mathbf{v}_i$. Substitute in:
 \begin{align*}
 \mathbf{e}_{k}  -  \mathbf{e}_{k + 1} &= \sum_i \gamma^k_i \lambda_i\mathbf{v}_i\\
 \mathbf{e}_{k + 1} &=  \mathbf{e}_{k} - \sum_i \gamma^k_i \lambda_i\mathbf{v}_i\\
  &= \sum_i \gamma^k_i\mathbf{v}_i - \sum_i \gamma^k_i\lambda_i\mathbf{v}_i \\
  &= \sum_i \gamma^k_i(1-\lambda_i)\mathbf{v}_i \\
 \end{align*}
 We can write in Eigen component:
  \begin{equation}
  \gamma^{k+1}_i = \gamma^k_i(1-\lambda_i)
  \end{equation}
  That is, this smoother scales the error by each Eigen component by the amount of $1-\lambda_i$. That is it can only converge if $\lambda_i < 1, \forall \text{ Eigen values } \lambda_i$. Closer $\lambda_i$ is to $1$, faster that error mode is reduced. Other smoothers suppress Eigen modes differently, but in general, they follow the same principle: They are more effective in reducing error modes associated with larger Eigen values. Therefore the error modes with smaller Eigen values should be captured by the coarse grid. Intuitively, the smallest Eigen vector that is not projected onto the coarse grid will be the bottleneck of the convergence, and dictates the asymptotic convergence rate. But the relation is not trivial. Given the coarse grid operator $L^c$ is a subspace of the fine grid operator $L^f$. If we can decompose an error vector $\mathbf{e}^f$ into two parts $\mathbf{e}^{fc}$, such that $\exists \mathbf{e}^{c}$, satisfies $\mathbf{P}\mathbf{e}^{c} = \mathbf{e}^{fc}$, and the remainder $\mathbf{e}^{ff} = \mathbf{e}^f - \mathbf{e}^{fc}$. i.e.:
  $$
  \mathbf{e}^f = \mathbf{e}^{fc} + \mathbf{e}^{ff}
  $$
 A good smoother in this case, would be able to effectively reduce error mode $\mathbf{e}^{ff}$,  even it may potentially increase error $\mathbf{e}^{fc}$. 
\section{Convergence Metrics} 
\label{sec:Convergence_Metric}
  To better quantify multigrid quality, mathematically, it can be measured by two metrics[\cite{brezina2001algebraic}]: 
\begin{equation}
\label{equ:M1}
M_1(\mathbf{Q},\mathbf{e}) := \frac{<(\mathbf{I}-\mathbf{Q})\mathbf{e}, (\mathbf{I}-\mathbf{Q})\mathbf{e}>}{<\mathbf{L}\mathbf{e},\mathbf{e}>}
\end{equation}
\begin{equation}
\label{equ:M2}
M_2(\mathbf{Q},\mathbf{e}) := \frac{<\mathbf{L}(\mathbf{I}-\mathbf{Q})\mathbf{e}, (\mathbf{I}-\mathbf{Q})\mathbf{e}>}{<\mathbf{L}\mathbf{e},\mathbf{e}>}
\end{equation}
$M_2$ is used in work by \cite{mccormick1984multigrid}, \cite{mccormick1982multigrid}, \cite{mccormick1985multigrid}. $M_1$ is introduced by \cite{brandt1986algebraic}. The metric measures how well the prolongation operator is for a given error $\mathbf{e}$. $\mathbf{Q}$ is a projection operator that projects a fine vector $\mathbf{e}^f$ onto a subspace that can be resolved by the coarse grid, $\mathbf{e}^{fc}$. 
$$
\mathbf{e}^{fc} = \mathbf{Q}\mathbf{e}^f
$$
Here is the formal definition: 
\begin{equation}
\mathbf{Q} = \mathbf{P}\mathbf{R}
\end{equation}
$\mathbf{P}$ is the prolongation operator defined in the previous sections. $\mathbf{R}$ is a restriction operator that computes a coarse vector $\mathbf{e}^c$ from a fine vecotor $\mathbf{e}^f$. In the context of Galerkin coarsening, if our fine grid solution is $\mathbf{u}^*$ with initial guess of $\mathbf{u}^0 = \mathbf{0}$. The coarse grid vector $\mathbf{u}^c$ can be computed as follows:
\begin{align*}
\mathbf{r}^f &= \mathbf{L}^f\mathbf{u}^* \\
\mathbf{b}^c &= \mathbf{P}^T\mathbf{r}^f \\
\mathbf{u}^c &= (\mathbf{L}^c)^{-1}\mathbf{b}^c \\
&=  (\mathbf{P}^T\mathbf{L}^f\mathbf{P})^{-1}\mathbf{b}^c \\
&=  (\mathbf{P}^T\mathbf{L}^f\mathbf{P})^{-1}\mathbf{P}^T\mathbf{r}^f \\
&= (\mathbf{P}^T\mathbf{L}^f\mathbf{P})^{-1}\mathbf{P}^T\mathbf{L}^f\mathbf{u}^* \\
\mathbf{u}^c &= \mathbf{R} \mathbf{u}^*
\end{align*}
So we can derive:
\begin{equation}
 \mathbf{R} = (\mathbf{P}^T\mathbf{L}^f\mathbf{P})^{-1}\mathbf{P}^T\mathbf{L}^f
\end{equation}
By using this definition of $\mathbf{R}$, we have $\mathbf{R}\mathbf{P} = \mathbf{I}_c$. $\mathbf{I}_c$ is the identity matrix of the same dimension of number of DOF in the coarse grid. That is, for error modes that can be resolved in the coarse grid, i.e. error modes can be written as $\mathbf{e}^f = \mathbf{P}\mathbf{e}^c$. We have:
\begin{align*}
\mathbf{Q}\mathbf{e}^f &= \mathbf{P}\mathbf{R}\mathbf{e}^f\\
&= \mathbf{P}\mathbf{R}\mathbf{P}\mathbf{e}^c\\
&= \mathbf{P}(\mathbf{P}^T\mathbf{L}^f\mathbf{P})^{-1}\mathbf{P}^T\mathbf{L}^f\mathbf{P}\mathbf{e}^c\\
&= \mathbf{P}(\mathbf{L}^c)^{-1}\mathbf{L}^c\mathbf{e}^c\\
&= \mathbf{P}\mathbf{I}\mathbf{e}^c\\
&= \mathbf{P}\mathbf{e}^c \\
&= \mathbf{e}^f
\end{align*}
We can see that 
\begin{equation}\label{equ:projection_Q}
(\mathbf{I} - \mathbf{Q})\mathbf{e}^f = \mathbf{0}\text{, if $\exists$ $\mathbf{e}^c$, s.t. }\mathbf{e}^f = \mathbf{P}\mathbf{e}^c
\end{equation}
But unfortunately, using this definition of $\mathbf{R}$ for analysis often is too computationally complex as it requires the inversion of the coarse level operator $(\mathbf{L}^c)^{-1}$. Therefore, in work by \cite{brezina2001algebraic}, an alternative definition of $\mathbf{R}$ is used:
\begin{equation}
\mathbf{R}_{ij} = \left\{  
\begin{aligned}
    1, & \text{ if}\ \mathbf{X}^f_j = \mathbf{X}^c_i\\
    0, & \text{ otherwise}
  \end{aligned}\right.
\end{equation}
There $\mathbf{R}$ is a simple injection. That is with value $1$, where the coarse node and fine node coincide in space, i.e. $\mathbf{X}^f_j = \mathbf{X}^c_i$, where $\mathbf{X}^f_j$ is the spacial location of fine node j and $\mathbf{X}^c_i$ is the spacial location of coarse node i.

\begin{lem}\label{lemma:injection}The injection based $\mathbf{R}$ creates a valid projection $\mathbf{Q}$ that satisfies Equation \ref{equ:projection_Q}.\end{lem}
\begin{proof}
We can rearrange the fine DOF such that fine DOF that does not spatially coincide with any coarse node first, then those that coincide with coarse node after. We can write the fine vector $\mathbf{e}$ as:
\begin{equation*}
\mathbf{e} = \left[ \begin{aligned}
    \mathbf{e}_{ff}\\
    \mathbf{e}_{fc}
  \end{aligned}\right]
\end{equation*}
Accordingly, $\mathbf{R}$ and $\mathbf{P}$ can be written as:
\begin{align*}
\mathbf{P} &= \left[ \begin{array}{c}
    \mathbf{P}_{cf}\\
    \mathbf{I}_{c}
  \end{array}\right]\\
  \mathbf{R} &= \left[ \begin{array}{cc}
    \mathbf{0} & \mathbf{I}_{c}
  \end{array}\right]\\
 \end{align*}
 $\mathbf{Q}$ therefore become:
 \begin{align*}
  \mathbf{Q} &= \mathbf{P}\mathbf{R} \\
  &= \left[ \begin{array}{cc}
    \mathbf{0} & \mathbf{P}_{cf} \\
    \mathbf{0} & \mathbf{I}_c
  \end{array}\right]\\
 \end{align*}
 Plug into Equation \ref{equ:projection_Q}:
 \begin{align*}
(\mathbf{I} - \mathbf{Q})\mathbf{e} &= \left[ \begin{array}{cc}
    \mathbf{I} & -\mathbf{P}_{cf} \\
    \mathbf{0} & \mathbf{0}
  \end{array}\right] \left[ \begin{array}{c}
    \mathbf{e}_{ff}\\
    \mathbf{e}_{fc}
  \end{array}\right]\\
  &= \mathbf{e}_{ff} - \mathbf{P}_{cf}\mathbf{e}_{fc} \numberthis \label{equ:proof_Q}
\end{align*}
If $\exists\mathbf{e}_{c}$ s.t. $\mathbf{e} = \mathbf{P}\mathbf{e}_c$:
 \begin{align*}
 \mathbf{P}\mathbf{e}_c &= \left[ \begin{array}{c}
    \mathbf{P}_{cf}\\
    \mathbf{I}_{c}
  \end{array}\right]\mathbf{e}_c\\
  &= \left[ \begin{array}{c}
    \mathbf{P}_{cf}\mathbf{e}_c\\
    \mathbf{e}_c
  \end{array}\right]\\
  &= \mathbf{e}\\
  &= \left[ \begin{array}{c}
    \mathbf{e}_{ff}\\
    \mathbf{e}_{fc}
  \end{array}\right]
 \end{align*}
 Therefore:
  \begin{align*}
  \mathbf{e}_{ff} &= \mathbf{P}_{cf}\mathbf{e}_c\\
  \mathbf{e}_{fc} &= \mathbf{e}_c
 \end{align*}
 Plug into Equation \ref{equ:proof_Q}:
   \begin{align*}
   (\mathbf{I} - \mathbf{Q})\mathbf{e} &= \mathbf{e}_{ff} - \mathbf{P}_{cf}\mathbf{e}_{fc}\\
  &= \mathbf{P}_{cf}\mathbf{e}_c - \mathbf{P}_{cf}\mathbf{e}_{fc}\\
  &= \mathbf{0}
 \end{align*}
 This concludes the proof.
\end{proof}
Lemma \ref{lemma:injection} states that though the $\mathbf{Q}$ defined above does not reflect the real process of acquiring the coarse grid correction, it is still a valid operator that project a fine vector $\mathbf{e}$ onto the range of the prolongation operator $\mathbf{P}$. The in the metrics $M_1$ and $M_2$, the inner product $<(\mathbf{I}-\mathbf{Q})\mathbf{e}, (\mathbf{I}-\mathbf{Q})\mathbf{e}>$ and $<\mathbf{L}(\mathbf{I}-\mathbf{Q})\mathbf{e}, (\mathbf{I}-\mathbf{Q})\mathbf{e}>$ measures how will the prolongation operator captures a given vector $\mathbf{e}$, either in $L^2$ norm or the energy norm $\mathbf{u}^T\mathbf{L}\mathbf{u}$. Note that if $\mathbf{e}$ can be captured by the coarse grid, both $M_1$ and $M_2$ are zero.  On the denominator $<\mathbf{L}\mathbf{e},\mathbf{e}>$ measures effectiveness of the smoother. In general, larger the energy norm of a vector is, more effective the smoother is. A simple example here: if $\mathbf{e}$ is one of the Eigen vector, the energy norm is square of the Eigen value associated with the Eigen vector. Therefore smaller $M_1$ and $M_2$ are, better the quality of prolongation is. So we can write the prolongation as a minimization problem:
\begin{equation}
\operatorname*{argmin}_{\mathbf{P}} \operatorname*{max}_{\mathbf{e}}M_i(\mathbf{P},\mathbf{e})\text{, }||\mathbf{e}||_2 = 1
\end{equation}
Here $i$ can be either $1$ or $2$ depends on which metric is used. But in reality other constraints will need to be imposed on $\mathbf{P}$ to limit its sparsity and the sparsity of a coarse level operator. In chapter \ref{Chapter:BBMG}, the construction of the prolongation $\mathbf{P}$ operator will be explored under the context of FEM discretized linear elasticity. But let's first examine how can we deploy multigrid onto modern hardware.
